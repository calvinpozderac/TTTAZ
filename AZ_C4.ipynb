{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calvinpozderac/AZ-testing/blob/az-c4/AZ_C4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Rw9u5MvSll",
        "outputId": "01db95d9-66c2-47dd-df2b-5227ae22ee0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.amp import autocast, GradScaler\n",
        "import math\n",
        "import random\n",
        "from collections import deque, defaultdict\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import copy\n",
        "import heapq\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npjJZOAAjpsx"
      },
      "outputs": [],
      "source": [
        "class ConnectFourGame:\n",
        "    \"\"\"Connect Four game implementation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.board_size = (self.rows, self.cols)\n",
        "        self.action_size = self.cols # Actions correspond to columns\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros(self.board_size, dtype=np.int8)\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        # Valid moves are columns that are not full (top row is 0)\n",
        "        return state[0, :] == 0\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "        next_state = state.copy()\n",
        "        # Find the lowest empty row in the chosen column\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            if next_state[r, action] == 0:\n",
        "                next_state[r, action] = player\n",
        "                break\n",
        "        return next_state, -player  # Switch player\n",
        "\n",
        "    def get_game_ended(self, state, player):\n",
        "        # Check for 4 in a row horizontally, vertically, and diagonally\n",
        "\n",
        "        # Check horizontal\n",
        "        for r in range(self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(state[r, c:c+4])) == 4:\n",
        "                    return sum(state[r, c:c+4]) / 4\n",
        "\n",
        "        # Check vertical\n",
        "        for c in range(self.cols):\n",
        "            for r in range(self.rows - 3):\n",
        "                if abs(sum(state[r:r+4, c])) == 4:\n",
        "                    return sum(state[r:r+4, c]) / 4\n",
        "\n",
        "        # Check diagonals (down-right)\n",
        "        for r in range(self.rows - 3):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(np.diagonal(state[r:r+4, c:c+4]))) == 4:\n",
        "                    return sum(np.diagonal(state[r:r+4, c:c+4])) / 4\n",
        "\n",
        "        # Check diagonals (up-right)\n",
        "        for r in range(3, self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(np.diagonal(np.fliplr(state[r-3:r+1, c:c+4])))) == 4:\n",
        "                    return sum(np.diagonal(np.fliplr(state[r-3:r+1, c:c+4]))) / 4\n",
        "\n",
        "        # Check for draw\n",
        "        if not (state[0, :] == 0).any(): # Check if the top row is full\n",
        "            return 0\n",
        "\n",
        "        return None  # Game continues\n",
        "\n",
        "    def get_canonical_form(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_symmetries(self, state, pi):\n",
        "        \"\"\"Get symmetries for Connect Four (just horizontal flip)\"\"\"\n",
        "        # In Connect Four, only horizontal flipping is a relevant symmetry\n",
        "        symmetries = []\n",
        "\n",
        "        # Original\n",
        "        symmetries.append((state, pi))\n",
        "\n",
        "        # Horizontal flip\n",
        "        state_flip = np.fliplr(state)\n",
        "        pi_flip = np.fliplr(pi.reshape(1, self.cols)).flatten()\n",
        "        symmetries.append((state_flip, pi_flip))\n",
        "\n",
        "        return symmetries\n",
        "\n",
        "    def string_representation(self, state):\n",
        "        \"\"\"Returns a string representation of the board\"\"\"\n",
        "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
        "        board_str = \"\"\n",
        "        for r in range(self.rows):\n",
        "            board_str += \" \".join([symbols[state[r, c]] for c in range(self.cols)]) + \"\\n\"\n",
        "        board_str += \"--------------------\\n\"\n",
        "        board_str += \"0 1 2 3 4 5 6\"\n",
        "        return board_str\n",
        "\n",
        "# --- Neural Network ---\n",
        "# To extend to a new game, adjust the network architecture:\n",
        "# - Input shape: Match the new game's board representation.\n",
        "# - Output shapes: Match the new game's action space (policy head) and value output (value head).\n",
        "# - Consider different convolutional filter sizes or types if the board structure is very different.\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"Optimized neural network for AlphaZero\"\"\"\n",
        "\n",
        "    def __init__(self, game, num_channels=32):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.board_size = game.board_size\n",
        "        self.action_size = game.action_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        # Convolutional layers (adjust kernel size/padding if board size changes significantly)\n",
        "        self.conv1 = nn.Conv2d(1, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn4 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "        # Policy head (adjust output size to game.action_size)\n",
        "        self.conv_policy = nn.Conv2d(num_channels, 32, 1)\n",
        "        self.bn_policy = nn.BatchNorm2d(32)\n",
        "        self.fc_policy = nn.Linear(32 * self.board_size[0] * self.board_size[1], self.action_size)\n",
        "\n",
        "        # Value head (adjust input size if board size changes)\n",
        "        self.conv_value = nn.Conv2d(num_channels, 3, 1)\n",
        "        self.bn_value = nn.BatchNorm2d(3)\n",
        "        self.fc_value1 = nn.Linear(3 * self.board_size[0] * self.board_size[1], 64)\n",
        "        self.fc_value2 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, s):\n",
        "        # Common layers\n",
        "        s = s.view(-1, 1, self.board_size[0], self.board_size[1])\n",
        "        s = F.relu(self.bn1(self.conv1(s)))\n",
        "        s = F.relu(self.bn2(self.conv2(s)))\n",
        "        s = F.relu(self.bn3(self.conv3(s)))\n",
        "        s = F.relu(self.bn4(self.conv4(s)))\n",
        "\n",
        "        # Policy head\n",
        "        pi = F.relu(self.bn_policy(self.conv_policy(s)))\n",
        "        pi = pi.view(pi.size(0), -1)\n",
        "        pi = self.dropout(pi)\n",
        "        pi = self.fc_policy(pi)\n",
        "        pi = F.log_softmax(pi, dim=1)\n",
        "\n",
        "        # Value head\n",
        "        v = F.relu(self.bn_value(self.conv_value(s)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = self.dropout(v)\n",
        "        v = F.relu(self.fc_value1(v))\n",
        "        v = self.fc_value2(v)\n",
        "        v = torch.tanh(v)\n",
        "\n",
        "        return pi, v\n",
        "\n",
        "# --- MCTS ---\n",
        "# The MCTS class is generally game-agnostic, relying on the Game and NeuralNetwork classes.\n",
        "# No major changes needed here for a new game unless the MCTS algorithm itself needs modification\n",
        "# (e.g., for games with very large branching factors).\n",
        "class MCTS:\n",
        "    \"\"\"Optimized Monte Carlo Tree Search\"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Qsa = {}  # Q values\n",
        "        self.Nsa = {}  # Visit counts for state-action\n",
        "        self.Ns = {}   # Visit counts for state\n",
        "        self.Ps = {}   # Policy\n",
        "        self.Es = {}   # Game ended\n",
        "        self.Vs = {}   # Valid moves\n",
        "\n",
        "    def get_action_prob(self, state, temp=1):\n",
        "        \"\"\"Get action probabilities and value estimate after MCTS simulations\"\"\"\n",
        "        for _ in range(self.args.num_mcts_sims):\n",
        "            self.search(state, 1)\n",
        "\n",
        "        s = self.game.get_canonical_form(state, 1)\n",
        "        # Use a more robust string representation for state keys\n",
        "        s_str = np.array2string(s, separator=',', max_line_width=np.inf)\n",
        "        counts = [self.Nsa.get((s_str, a), 0) for a in range(self.game.action_size)]\n",
        "\n",
        "        if temp == 0:\n",
        "            # Deterministic - choose best action\n",
        "            best_actions = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
        "            best_action = np.random.choice(best_actions)\n",
        "            probs = np.zeros(len(counts))\n",
        "            probs[best_action] = 1\n",
        "        else:\n",
        "            # Probabilistic based on temperature\n",
        "            counts = np.array(counts, dtype=np.float64)\n",
        "            counts = counts ** (1.0 / temp)\n",
        "            probs = counts / counts.sum()\n",
        "\n",
        "        # Calculate MCTS value estimate\n",
        "        # Weighted average of action values based on visit counts\n",
        "        total_visits = sum(counts)\n",
        "        if total_visits > 0:\n",
        "            mcts_value = 0\n",
        "            for a in range(self.game.action_size):\n",
        "                if (s_str, a) in self.Qsa and counts[a] > 0:\n",
        "                    mcts_value += (counts[a] / total_visits) * self.Qsa[(s_str, a)]\n",
        "        else:\n",
        "            mcts_value = 0\n",
        "\n",
        "        return probs, mcts_value\n",
        "\n",
        "    def search(self, state, player):\n",
        "        \"\"\"MCTS search with neural network guidance\"\"\"\n",
        "        s = self.game.get_canonical_form(state, player)\n",
        "        # Use a more robust string representation for state keys\n",
        "        s_str = np.array2string(s, separator=',', max_line_width=np.inf)\n",
        "\n",
        "\n",
        "        if s_str not in self.Es:\n",
        "            self.Es[s_str] = self.game.get_game_ended(s, 1) # Evaluate from player 1's perspective\n",
        "\n",
        "        if self.Es[s_str] is not None:\n",
        "            # Terminal node\n",
        "            return self.Es[s_str] # Return the game result directly\n",
        "\n",
        "        if s_str not in self.Ps:\n",
        "            # Leaf node - expand using neural network\n",
        "            with torch.no_grad():\n",
        "                # Ensure input tensor has the correct shape (batch, channels, height, width)\n",
        "                s_tensor = torch.FloatTensor(s).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                log_pi, v = self.nnet(s_tensor)\n",
        "                pi = torch.exp(log_pi).cpu().numpy()[0]\n",
        "\n",
        "            valid_moves = self.game.get_valid_moves(s)\n",
        "            pi = pi * valid_moves  # Mask invalid moves\n",
        "            pi_sum = np.sum(pi)\n",
        "            if pi_sum > 0:\n",
        "                pi = pi / pi_sum  # Renormalize\n",
        "            else:\n",
        "                # If all valid moves have zero probability, distribute probability evenly\n",
        "                pi = valid_moves / np.sum(valid_moves)\n",
        "\n",
        "            self.Ps[s_str] = pi\n",
        "            self.Vs[s_str] = valid_moves\n",
        "            self.Ns[s_str] = 0\n",
        "            return v.item() # Return the value estimate\n",
        "\n",
        "        # Internal node - select action using UCB\n",
        "        valid_moves = self.Vs[s_str]\n",
        "        cur_best = -float('inf')\n",
        "        best_act = -1\n",
        "\n",
        "        # UCB formula\n",
        "        for a in range(self.game.action_size):\n",
        "            if valid_moves[a]:\n",
        "                if (s_str, a) in self.Qsa:\n",
        "                    u = (self.Qsa[(s_str, a)] +\n",
        "                         self.args.cpuct * self.Ps[s_str][a] *\n",
        "                         math.sqrt(self.Ns[s_str]) / (1 + self.Nsa[(s_str, a)]))\n",
        "                else:\n",
        "                    u = (self.args.cpuct * self.Ps[s_str][a] *\n",
        "                         math.sqrt(self.Ns[s_str] + 1e-8)) # Add epsilon to prevent division by zero\n",
        "\n",
        "                if u > cur_best:\n",
        "                    cur_best = u\n",
        "                    best_act = a\n",
        "\n",
        "        # Make move and recurse\n",
        "        next_s, next_player = self.game.get_next_state(state, best_act, player) # Use original state here\n",
        "        v = self.search(next_s, next_player)\n",
        "\n",
        "        # Backup\n",
        "        if (s_str, best_act) in self.Qsa:\n",
        "            self.Qsa[(s_str, best_act)] = ((self.Nsa[(s_str, best_act)] *\n",
        "                                          self.Qsa[(s_str, best_act)] + v) /\n",
        "                                         (self.Nsa[(s_str, best_act)] + 1))\n",
        "            self.Nsa[(s_str, best_act)] += 1\n",
        "        else:\n",
        "            self.Qsa[(s_str, best_act)] = v\n",
        "            self.Nsa[(s_str, best_act)] = 1\n",
        "\n",
        "        self.Ns[s_str] += 1\n",
        "        return v\n",
        "\n",
        "# --- AlphaZero Trainer ---\n",
        "# The trainer orchestrates self-play, training, and evaluation.\n",
        "# Mostly game-agnostic, but requires the Game, NeuralNetwork, MCTS, and Args classes.\n",
        "class AlphaZeroTrainer:\n",
        "    \"\"\"Main AlphaZero trainer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.game = ConnectFourGame()\n",
        "        self.args = Args()\n",
        "        self.nnet = NeuralNetwork(self.game).to(device)\n",
        "        self.optimizer = optim.Adam(self.nnet.parameters(), lr=self.args.lr,\n",
        "                                   weight_decay=self.args.weight_decay)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
        "        self.scaler = GradScaler('cuda')\n",
        "\n",
        "        self.train_examples_history = deque([], maxlen=self.args.max_memory_size // self.args.num_eps) # Store examples from recent episodes\n",
        "\n",
        "        # Heap: (-elo, iteration, model_copy)\n",
        "        self.model_heap = []\n",
        "        heapq.heappush(self.model_heap, (-1000, 0, copy.deepcopy(self.nnet)))  # Iter 0, base net\n",
        "\n",
        "\n",
        "    def execute_episode(self):\n",
        "        \"\"\"Execute one episode of self-play (fast version)\"\"\"\n",
        "        train_examples = []\n",
        "        state = self.game.get_initial_state()\n",
        "        current_player = 1\n",
        "        episode_step = 0\n",
        "\n",
        "        # Create a single MCTS instance for the whole episode\n",
        "        mcts = MCTS(self.game, self.nnet, self.args)\n",
        "\n",
        "        while True:\n",
        "            episode_step += 1\n",
        "            canonical_state = self.game.get_canonical_form(state, current_player)\n",
        "            temp = int(episode_step < self.args.temp_threshold)\n",
        "\n",
        "            # Get action probabilities from MCTS\n",
        "            pi, _ = mcts.get_action_prob(canonical_state, temp=temp)\n",
        "\n",
        "            # Store training example (value target will be filled in later)\n",
        "            sym = self.game.get_symmetries(canonical_state, pi)\n",
        "            for s, p in sym:\n",
        "                train_examples.append([s, current_player, p])  # no value yet\n",
        "\n",
        "            # Sample action from probabilities\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "\n",
        "            # Make move\n",
        "            state, current_player = self.game.get_next_state(state, action, current_player)\n",
        "\n",
        "            # Check if game ended\n",
        "            result = self.game.get_game_ended(state, current_player)\n",
        "            if result is not None:\n",
        "                # Assign final outcome z to all training examples\n",
        "                final_examples = []\n",
        "                for s, player, p in train_examples:\n",
        "                    z = result * player  # flip perspective depending on who played\n",
        "                    final_examples.append([s, player, p, z])\n",
        "                return final_examples\n",
        "\n",
        "    def train(self, examples):\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        random.shuffle(examples)\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        self.nnet.train() # Set model to training mode\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            batch_idx = 0\n",
        "\n",
        "            while batch_idx < len(examples):\n",
        "                batch_examples = examples[batch_idx:batch_idx + self.args.batch_size]\n",
        "                batch_idx += self.args.batch_size\n",
        "\n",
        "                # Ensure input tensor has the correct shape (batch, channels, height, width)\n",
        "                boards = torch.FloatTensor(np.array([ex[0] for ex in batch_examples])).unsqueeze(1).to(device)\n",
        "                target_pis = torch.FloatTensor(np.array([ex[2] for ex in batch_examples])).to(device)\n",
        "                target_vs = torch.FloatTensor(np.array([ex[3] for ex in batch_examples])).to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                with autocast(device_type='cuda'):\n",
        "                    out_pi, out_v = self.nnet(boards)\n",
        "                    l_pi = self.loss_pi(target_pis, out_pi)\n",
        "                    l_v = self.loss_v(target_vs, out_v)\n",
        "                    total_loss_batch = l_pi + l_v\n",
        "\n",
        "                self.scaler.scale(total_loss_batch).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                total_loss += total_loss_batch.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "        avg_loss = total_loss / num_batches\n",
        "\n",
        "        self.nnet.eval()\n",
        "        return avg_loss\n",
        "\n",
        "    def loss_pi(self, targets, outputs):\n",
        "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
        "\n",
        "    def loss_v(self, targets, outputs):\n",
        "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
        "\n",
        "    def play_game(self, nnet1, nnet2):\n",
        "        \"\"\"Play a single game between two nets (fast version)\"\"\"\n",
        "        state = self.game.get_initial_state()\n",
        "        player = 1\n",
        "\n",
        "        # Create MCTS instances once per player\n",
        "        mcts1 = MCTS(self.game, nnet1, self.args)\n",
        "        mcts2 = MCTS(self.game, nnet2, self.args)\n",
        "\n",
        "        while True:\n",
        "            nnet = nnet1 if player == 1 else nnet2\n",
        "            mcts = mcts1 if player == 1 else mcts2\n",
        "\n",
        "            pi, _ = mcts.get_action_prob(self.game.get_canonical_form(state, player), temp=0)\n",
        "            action = np.argmax(pi)\n",
        "\n",
        "            state, player = self.game.get_next_state(state, action, player)\n",
        "\n",
        "            result = self.game.get_game_ended(state, player)\n",
        "            if result is not None:\n",
        "                if result == 0:\n",
        "                    return 0.5\n",
        "                return 1 if result == 1 else 0\n",
        "\n",
        "    def update_elo(self, elo_a, elo_b, score_a, k=32):\n",
        "        \"\"\"Update Elo ratings after a game. score_a is 1 if A wins, 0.5 for draw, 0 if A loses.\"\"\"\n",
        "        expected_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))\n",
        "        expected_b = 1 - expected_a\n",
        "        new_elo_a = elo_a + k * (score_a - expected_a)\n",
        "        new_elo_b = elo_b + k * ((1 - score_a) - expected_b)\n",
        "        return new_elo_a, new_elo_b\n",
        "\n",
        "    def evaluate_models(self, new_model, iteration):\n",
        "        # Pop all previous models from the heap\n",
        "        previous_models = []\n",
        "        while self.model_heap:\n",
        "            previous_models.append(heapq.heappop(self.model_heap))\n",
        "\n",
        "        new_elo = 1000  # starting Elo for new model\n",
        "\n",
        "        # Play two games against each previous model: once as first, once as second\n",
        "        for i, (neg_elo, iter_idx, old_model) in enumerate(previous_models):\n",
        "            # Game 1: new_model goes first\n",
        "            score1 = self.play_game(new_model, old_model)\n",
        "            new_elo, old_elo = self.update_elo(new_elo, -neg_elo, score1)\n",
        "\n",
        "            # Game 2: new_model goes second\n",
        "            score2 = self.play_game(old_model, new_model)\n",
        "            # Swap score for perspective of new_model\n",
        "            new_elo, old_elo = self.update_elo(new_elo, old_elo, 1 - score2)\n",
        "\n",
        "            # Update the heap tuple with the new opponent Elo\n",
        "            previous_models[i] = (-old_elo, iter_idx, old_model)\n",
        "\n",
        "        # Push all previous models back into the heap\n",
        "        for model in previous_models:\n",
        "            heapq.heappush(self.model_heap, model)\n",
        "\n",
        "        # Push new model into heap\n",
        "        heapq.heappush(self.model_heap, (-new_elo, iteration, new_model))\n",
        "\n",
        "        # Optionally, set self.nnet to the model with highest Elo\n",
        "        top_neg_elo, _, top_model = max(self.model_heap)\n",
        "        self.nnet = top_model\n",
        "\n",
        "        return new_elo\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting AlphaZero training...\")\n",
        "\n",
        "        for i in range(1, self.args.num_iters + 1):\n",
        "            print(f\"\\n=== Iteration {i}/{self.args.num_iters} ===\")\n",
        "\n",
        "            # Self-play\n",
        "            iteration_train_examples = deque([], maxlen=self.args.max_memory_size)\n",
        "\n",
        "            print(\"Collecting self-play games...\")\n",
        "            for eps in range(self.args.num_eps):\n",
        "                iteration_train_examples.extend(self.execute_episode()) # Use extend\n",
        "                if (eps + 1) % 10 == 0: # Print progress less frequently\n",
        "                    print(f\"Episode {eps + 1}/{self.args.num_eps}\")\n",
        "\n",
        "            # Add to training history\n",
        "            self.train_examples_history.append(iteration_train_examples)\n",
        "\n",
        "            # Prepare training data from history\n",
        "            train_examples = []\n",
        "            for e in self.train_examples_history:\n",
        "                train_examples.extend(e)\n",
        "\n",
        "            # Train\n",
        "            print(f\"Training on {len(train_examples)} examples...\")\n",
        "            avg_loss = self.train(train_examples)\n",
        "\n",
        "            # Save and evaluate\n",
        "            new_model = copy.deepcopy(self.nnet)\n",
        "            elo = self.evaluate_models(new_model, i)\n",
        "\n",
        "            print(\"\\n--- Elo Leaderboard ---\")\n",
        "            # Convert heap into a sorted list by Elo (highest first)\n",
        "            sorted_models = sorted(self.model_heap, key=lambda x: x[0])  # x[0] is -elo\n",
        "            for neg_elo, iteration, _ in sorted_models:\n",
        "                print(f\"Iteration {iteration}: {-neg_elo:.2f}\")\n",
        "            print(\"-----------------------\\n\")\n",
        "\n",
        "            # Pick the best model (highest Elo = smallest neg_elo in heap)\n",
        "            neg_elo, best_iter, best_model = self.model_heap[0]\n",
        "\n",
        "            # Update current nnet to the best model\n",
        "            #self.nnet = best_model\n",
        "\n",
        "            # Reinitialize optimizer and scaler for the new model\n",
        "            self.optimizer = torch.optim.Adam(self.nnet.parameters(), lr=self.args.lr)\n",
        "            self.scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model_heap\n",
        "\n",
        "def demo_game():\n",
        "    \"\"\"Demo game between trained AlphaZero and Optimal AI and evaluate specific state\"\"\"\n",
        "    print(\"\\n=== Demo Game and Evaluation ===\")\n",
        "    trainer = AlphaZeroTrainer()\n",
        "\n",
        "    # Quick training for demo (reduce iterations for faster demo)\n",
        "    trainer.args.num_iters = 100 # Reduced iterations\n",
        "    trainer.args.num_eps = 50 # Reduced episodes\n",
        "    trainer.args.num_mcts_sims = 100 # Reduced simulations per move\n",
        "\n",
        "    print(\"Training AlphaZero (reduced for demo)...\")\n",
        "    model_heap = trainer.learn()\n",
        "\n",
        "    return model_heap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self = AlphaZeroTrainer()\n",
        "\n",
        "iteration_train_examples = deque([], maxlen=self.args.max_memory_size)\n",
        "\n",
        "\n",
        "self.args.num_eps = 50 # Reduced episodes\n",
        "self.args.num_mcts_sims = 100 # Reduced simulations per move\n",
        "\n",
        "print(\"Collecting self-play games...\")\n",
        "for eps in range(self.args.num_eps):\n",
        "    iteration_train_examples.extend(self.execute_episode()) # Use extend\n",
        "    if (eps + 1) % 10 == 0: # Print progress less frequently\n",
        "        print(f\"Episode {eps + 1}/{self.args.num_eps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn4Oacb4TsY_",
        "outputId": "4369139b-1b00-4518-fb7f-26b1c04f27f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting self-play games...\n",
            "Episode 10/50\n",
            "Episode 20/50\n",
            "Episode 30/50\n",
            "Episode 40/50\n",
            "Episode 50/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Collecting self-play games...\")\n",
        "for eps in range(self.args.num_eps):\n",
        "    iteration_train_examples.extend(self.execute_episode()) # Use extend\n",
        "    if (eps + 1) % 10 == 0: # Print progress less frequently\n",
        "        print(f\"Episode {eps + 1}/{self.args.num_eps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDk8fCSmWBTO",
        "outputId": "faf85873-0eb5-4d45-de60-34caf340f4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting self-play games...\n",
            "Episode 10/50\n",
            "Episode 20/50\n",
            "Episode 30/50\n",
            "Episode 40/50\n",
            "Episode 50/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = []\n",
        "for e in iteration_train_examples:\n",
        "    train_examples.extend(e)"
      ],
      "metadata": {
        "id": "QP3ed1qzbWeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iteration_train_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjTFTYy0bzhJ",
        "outputId": "2fe1cba2-0e55-4b0b-9351-4b7e89093166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0,  0,  0,  0,  0,  0],\n",
              "        [ 0,  0, -1,  0,  0,  0,  0],\n",
              "        [ 1,  1, -1,  1, -1, -1,  0]], dtype=int8),\n",
              " -1,\n",
              " array([0.17322835, 0.11811024, 0.14173228, 0.08661417, 0.24409449,\n",
              "        0.11023622, 0.12598425]),\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_nns = []\n",
        "names = []\n",
        "for num in [10/150,25/150,50/150,75/150,100/150,125/150,150/150]:\n",
        "    for num_steps in [25,50,75,100,200]:\n",
        "        for lr in [0.001, 0.005, 0.01]:\n",
        "            nnet = NeuralNetwork(self.game).to(device)\n",
        "            optimizer = optim.Adam(nnet.parameters(), lr=lr,\n",
        "                                    weight_decay=self.args.weight_decay)\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=1)\n",
        "            scaler = GradScaler('cuda')\n",
        "\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            nnet.train() # Set model to training mode\n",
        "            random.shuffle(iteration_train_examples)\n",
        "\n",
        "            for epoch in range(num_steps):\n",
        "                batch_idx = 0\n",
        "\n",
        "                while batch_idx < len(iteration_train_examples)*num:\n",
        "                    batch_examples = list(iteration_train_examples)[batch_idx:batch_idx + self.args.batch_size]\n",
        "                    batch_idx += self.args.batch_size\n",
        "\n",
        "                    # Ensure input tensor has the correct shape (batch, channels, height, width)\n",
        "                    boards = torch.FloatTensor(np.array([ex[0] for ex in batch_examples])).unsqueeze(1).to(device)\n",
        "                    target_pis = torch.FloatTensor(np.array([ex[2] for ex in batch_examples])).to(device)\n",
        "                    target_vs = torch.FloatTensor(np.array([ex[3] for ex in batch_examples])).to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    with autocast(device_type='cuda'):\n",
        "                        out_pi, out_v = nnet(boards)\n",
        "                        l_pi = self.loss_pi(target_pis, out_pi)\n",
        "                        l_v = self.loss_v(target_vs, out_v)\n",
        "                        total_loss_batch = l_pi + l_v\n",
        "\n",
        "                    scaler.scale(total_loss_batch).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "\n",
        "                    total_loss += total_loss_batch.item()\n",
        "                    num_batches += 1\n",
        "            names.append(f\"num={int(num*150)}, num_steps={num_steps}, lr={lr}\")\n",
        "            print(names[-1])\n",
        "            all_nns.append(copy.deepcopy(nnet))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXv-bq20X9pw",
        "outputId": "55104fc2-ad41-4603-d2f6-627ccbd09af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num=10, num_steps=25, lr=0.001\n",
            "num=10, num_steps=25, lr=0.005\n",
            "num=10, num_steps=25, lr=0.01\n",
            "num=10, num_steps=50, lr=0.001\n",
            "num=10, num_steps=50, lr=0.005\n",
            "num=10, num_steps=50, lr=0.01\n",
            "num=10, num_steps=75, lr=0.001\n",
            "num=10, num_steps=75, lr=0.005\n",
            "num=10, num_steps=75, lr=0.01\n",
            "num=10, num_steps=100, lr=0.001\n",
            "num=10, num_steps=100, lr=0.005\n",
            "num=10, num_steps=100, lr=0.01\n",
            "num=10, num_steps=200, lr=0.001\n",
            "num=10, num_steps=200, lr=0.005\n",
            "num=10, num_steps=200, lr=0.01\n",
            "num=25, num_steps=25, lr=0.001\n",
            "num=25, num_steps=25, lr=0.005\n",
            "num=25, num_steps=25, lr=0.01\n",
            "num=25, num_steps=50, lr=0.001\n",
            "num=25, num_steps=50, lr=0.005\n",
            "num=25, num_steps=50, lr=0.01\n",
            "num=25, num_steps=75, lr=0.001\n",
            "num=25, num_steps=75, lr=0.005\n",
            "num=25, num_steps=75, lr=0.01\n",
            "num=25, num_steps=100, lr=0.001\n",
            "num=25, num_steps=100, lr=0.005\n",
            "num=25, num_steps=100, lr=0.01\n",
            "num=25, num_steps=200, lr=0.001\n",
            "num=25, num_steps=200, lr=0.005\n",
            "num=25, num_steps=200, lr=0.01\n",
            "num=50, num_steps=25, lr=0.001\n",
            "num=50, num_steps=25, lr=0.005\n",
            "num=50, num_steps=25, lr=0.01\n",
            "num=50, num_steps=50, lr=0.001\n",
            "num=50, num_steps=50, lr=0.005\n",
            "num=50, num_steps=50, lr=0.01\n",
            "num=50, num_steps=75, lr=0.001\n",
            "num=50, num_steps=75, lr=0.005\n",
            "num=50, num_steps=75, lr=0.01\n",
            "num=50, num_steps=100, lr=0.001\n",
            "num=50, num_steps=100, lr=0.005\n",
            "num=50, num_steps=100, lr=0.01\n",
            "num=50, num_steps=200, lr=0.001\n",
            "num=50, num_steps=200, lr=0.005\n",
            "num=50, num_steps=200, lr=0.01\n",
            "num=75, num_steps=25, lr=0.001\n",
            "num=75, num_steps=25, lr=0.005\n",
            "num=75, num_steps=25, lr=0.01\n",
            "num=75, num_steps=50, lr=0.001\n",
            "num=75, num_steps=50, lr=0.005\n",
            "num=75, num_steps=50, lr=0.01\n",
            "num=75, num_steps=75, lr=0.001\n",
            "num=75, num_steps=75, lr=0.005\n",
            "num=75, num_steps=75, lr=0.01\n",
            "num=75, num_steps=100, lr=0.001\n",
            "num=75, num_steps=100, lr=0.005\n",
            "num=75, num_steps=100, lr=0.01\n",
            "num=75, num_steps=200, lr=0.001\n",
            "num=75, num_steps=200, lr=0.005\n",
            "num=75, num_steps=200, lr=0.01\n",
            "num=100, num_steps=25, lr=0.001\n",
            "num=100, num_steps=25, lr=0.005\n",
            "num=100, num_steps=25, lr=0.01\n",
            "num=100, num_steps=50, lr=0.001\n",
            "num=100, num_steps=50, lr=0.005\n",
            "num=100, num_steps=50, lr=0.01\n",
            "num=100, num_steps=75, lr=0.001\n",
            "num=100, num_steps=75, lr=0.005\n",
            "num=100, num_steps=75, lr=0.01\n",
            "num=100, num_steps=100, lr=0.001\n",
            "num=100, num_steps=100, lr=0.005\n",
            "num=100, num_steps=100, lr=0.01\n",
            "num=100, num_steps=200, lr=0.001\n",
            "num=100, num_steps=200, lr=0.005\n",
            "num=100, num_steps=200, lr=0.01\n",
            "num=125, num_steps=25, lr=0.001\n",
            "num=125, num_steps=25, lr=0.005\n",
            "num=125, num_steps=25, lr=0.01\n",
            "num=125, num_steps=50, lr=0.001\n",
            "num=125, num_steps=50, lr=0.005\n",
            "num=125, num_steps=50, lr=0.01\n",
            "num=125, num_steps=75, lr=0.001\n",
            "num=125, num_steps=75, lr=0.005\n",
            "num=125, num_steps=75, lr=0.01\n",
            "num=125, num_steps=100, lr=0.001\n",
            "num=125, num_steps=100, lr=0.005\n",
            "num=125, num_steps=100, lr=0.01\n",
            "num=125, num_steps=200, lr=0.001\n",
            "num=125, num_steps=200, lr=0.005\n",
            "num=125, num_steps=200, lr=0.01\n",
            "num=150, num_steps=25, lr=0.001\n",
            "num=150, num_steps=25, lr=0.005\n",
            "num=150, num_steps=25, lr=0.01\n",
            "num=150, num_steps=50, lr=0.001\n",
            "num=150, num_steps=50, lr=0.005\n",
            "num=150, num_steps=50, lr=0.01\n",
            "num=150, num_steps=75, lr=0.001\n",
            "num=150, num_steps=75, lr=0.005\n",
            "num=150, num_steps=75, lr=0.01\n",
            "num=150, num_steps=100, lr=0.001\n",
            "num=150, num_steps=100, lr=0.005\n",
            "num=150, num_steps=100, lr=0.01\n",
            "num=150, num_steps=200, lr=0.001\n",
            "num=150, num_steps=200, lr=0.005\n",
            "num=150, num_steps=200, lr=0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elos = [1000 for _ in all_nns]\n",
        "ijs = [(i,j) for i in range(len(all_nns)) for j in range(i+1, len(all_nns))]\n",
        "for _ in range(10):\n",
        "    random.shuffle(ijs)\n",
        "    for i,j in ijs:\n",
        "        nnet1 = all_nns[i]\n",
        "        nnet2 = all_nns[j]\n",
        "        elo1 = elos[i]\n",
        "        elo2 = elos[j]\n",
        "\n",
        "        nnet1.eval()\n",
        "        nnet2.eval()\n",
        "        score1 = self.play_game(nnet1,nnet2)\n",
        "        elo1, elo2 = self.update_elo(elo1, elo2, score1)\n",
        "\n",
        "        # Game 2: new_model goes second\n",
        "        score2 = self.play_game(nnet2, nnet1)\n",
        "        # Swap score for perspective of new_model\n",
        "        elo1, elo2 = self.update_elo(elo1, elo2, 1 - score2)\n",
        "\n",
        "        elos[i] = elo1\n",
        "        elos[j] = elo2\n",
        "    for k in np.argsort(-np.array(elos))[:10]:\n",
        "        print(names[k], elos[k])"
      ],
      "metadata": {
        "id": "XUxWynm7cCi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14prAVYId2yJ"
      },
      "outputs": [],
      "source": [
        "# --- Training Arguments ---\n",
        "# Adjust these parameters based on the complexity of the new game.\n",
        "# More complex games may require more simulations, episodes, iterations, and a larger batch size.\n",
        "class Args:\n",
        "    \"\"\"Training arguments\"\"\"\n",
        "    def __init__(self):\n",
        "        # Self-play\n",
        "        self.num_iters = 100\n",
        "        self.num_eps = 50  # Episodes per iteration\n",
        "        self.temp_threshold = 15 # Number of moves to use temperature sampling in self-play\n",
        "\n",
        "        # MCTS\n",
        "        self.num_mcts_sims = 5 # Number of MCTS simulations per move\n",
        "        self.cpuct = 1.0 # Exploration constant\n",
        "\n",
        "        # Training\n",
        "        self.epochs = 50\n",
        "        self.batch_size = 256\n",
        "        self.lr = 0.01\n",
        "        self.dropout = 0.3\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "        # Memory\n",
        "        self.max_memory_size = 100000 # Maximum number of training examples\n",
        "\n",
        "        # Evaluation\n",
        "        self.arena_compare = 40 # Number of games to compare new vs old network\n",
        "        self.update_threshold = 0.6 # Minimum win rate against old network to update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXKHqICBwMw2"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    demo_game()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYtziK4ovQmz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMnW9FKN7ydpr2EKkPMkFb8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}