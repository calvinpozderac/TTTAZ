{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxJ7tPpVYbWsTZ0k7VSS6F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calvinpozderac/TTTAZ/blob/main/TTT_AZ_C4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.amp import autocast, GradScaler\n",
        "import math\n",
        "import random\n",
        "from collections import deque, defaultdict\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import copy\n",
        "import heapq\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Rw9u5MvSll",
        "outputId": "06484caf-01da-4fac-a859-c63cf8468d19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optimal AI ---\n",
        "# Implementing a perfect AI for a new game might be challenging or impossible\n",
        "# for complex games due to the state space size. For simple games like Tic-Tac-Toe,\n",
        "# a minimax implementation is feasible for evaluation.\n",
        "class OptimalAI:\n",
        "    \"\"\"Perfect minimax AI for Tic-Tac-Toe with caching\"\"\"\n",
        "\n",
        "    def __init__(self, game):\n",
        "        self.game = game\n",
        "        self.minimax_cache = {}  # Cache for minimax values\n",
        "        self.move_cache = {}     # Cache for best moves\n",
        "        self.precompute_all_positions()\n",
        "\n",
        "    def precompute_all_positions(self):\n",
        "        \"\"\"Precompute all possible positions for instant lookup\"\"\"\n",
        "        print(\"Precomputing optimal moves for all positions...\")\n",
        "\n",
        "        def generate_all_states(state, player):\n",
        "            \"\"\"Recursively generate and evaluate all possible game states\"\"\"\n",
        "            # Use a more robust string representation for state keys\n",
        "            state_key = (np.array2string(state.flatten(), separator=',', max_line_width=np.inf), player)\n",
        "\n",
        "            if state_key in self.minimax_cache:\n",
        "                return\n",
        "\n",
        "            # Check if game is over\n",
        "            result = self.game.get_game_ended(state, 1) # Evaluate from player 1's perspective\n",
        "            if result is not None:\n",
        "                # Store the result from the perspective of the current player\n",
        "                self.minimax_cache[state_key] = result * player\n",
        "                return\n",
        "\n",
        "            # Find best move and value for this position\n",
        "            best_score = float('-inf') if player == 1 else float('inf')\n",
        "            best_move = None\n",
        "\n",
        "            for action in range(9):\n",
        "                if state.flatten()[action] == 0:\n",
        "                    next_state, next_player = self.game.get_next_state(state, action, player)\n",
        "\n",
        "                    # Recursively evaluate next state\n",
        "                    generate_all_states(next_state, next_player)\n",
        "\n",
        "                    next_state_key = (np.array2string(next_state.flatten(), separator=',', max_line_width=np.inf), next_player)\n",
        "                    score = self.minimax_cache[next_state_key] * next_player # Get score from current player's perspective\n",
        "\n",
        "                    if player == 1 and score > best_score:\n",
        "                        best_score = score\n",
        "                        best_move = action\n",
        "                    elif player == -1 and score < best_score:\n",
        "                        best_score = score\n",
        "                        best_move = action\n",
        "\n",
        "            # Cache the results\n",
        "            self.minimax_cache[state_key] = best_score\n",
        "            self.move_cache[state_key] = best_move\n",
        "\n",
        "        # Generate all states starting from empty board\n",
        "        initial_state = self.game.get_initial_state()\n",
        "        generate_all_states(initial_state, 1)\n",
        "\n",
        "        print(f\"Precomputed {len(self.move_cache)} positions\")\n",
        "\n",
        "    def minimax(self, state, depth, maximizing_player):\n",
        "        \"\"\"Minimax with caching (alpha-beta pruning removed)\"\"\"\n",
        "        # Use a more robust string representation for state keys\n",
        "        state_key = (np.array2string(state.flatten(), separator=',', max_line_width=np.inf), 1 if maximizing_player else -1)\n",
        "\n",
        "\n",
        "        if state_key in self.minimax_cache:\n",
        "            return self.minimax_cache[state_key]\n",
        "\n",
        "        result = self.game.get_game_ended(state, 1 if maximizing_player else -1)\n",
        "\n",
        "        if result is not None:\n",
        "            self.minimax_cache[state_key] = result * (1 if maximizing_player else -1) # Store from current player's perspective\n",
        "            return self.minimax_cache[state_key]\n",
        "\n",
        "        if maximizing_player:\n",
        "            max_eval = float('-inf')\n",
        "            for action in range(9):\n",
        "                if state.flatten()[action] == 0:\n",
        "                    next_state, _ = self.game.get_next_state(state, action, 1)\n",
        "                    eval_score = self.minimax(next_state, depth + 1, False)\n",
        "                    max_eval = max(max_eval, eval_score)\n",
        "            self.minimax_cache[state_key] = max_eval\n",
        "            return max_eval\n",
        "        else:\n",
        "            min_eval = float('inf')\n",
        "            for action in range(9):\n",
        "                if state.flatten()[action] == 0:\n",
        "                    next_state, _ = self.game.get_next_state(state, action, -1)\n",
        "                    eval_score = self.minimax(next_state, depth + 1, True)\n",
        "                    min_eval = min(min_eval, eval_score)\n",
        "            self.minimax_cache[state_key] = min_eval\n",
        "            return min_eval\n",
        "\n",
        "    def get_best_move(self, state, player):\n",
        "        \"\"\"Get the optimal move using cached results\"\"\"\n",
        "        # Use a more robust string representation for state keys\n",
        "        state_key = (np.array2string(state.flatten(), separator=',', max_line_width=np.inf), player)\n",
        "\n",
        "\n",
        "        # Use precomputed move if available\n",
        "        if state_key in self.move_cache:\n",
        "            return self.move_cache[state_key]\n",
        "\n",
        "        # Fallback to real-time computation (shouldn't happen after precompute)\n",
        "        print(f\"Warning: Position {state_key} not in cache, computing real-time for player {player}\")\n",
        "        best_score = float('-inf') if player == 1 else float('inf')\n",
        "        best_move = None\n",
        "\n",
        "        for action in range(9):\n",
        "            if state.flatten()[action] == 0:\n",
        "                next_state, _ = self.game.get_next_state(state, action, player)\n",
        "                score = self.minimax(next_state, 0, player == 1) # Use player == 1 to get score from player 1's perspective\n",
        "\n",
        "                if player == 1 and score > best_score:\n",
        "                    best_score = score\n",
        "                    best_move = action\n",
        "                elif player == -1 and score < best_score:\n",
        "                    best_score = score\n",
        "                    best_move = action\n",
        "\n",
        "        return best_move"
      ],
      "metadata": {
        "id": "Us82vg6IvSOG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Game Class ---\n",
        "# To extend to a new game, create a new class inheriting from a base Game class\n",
        "# (or simply implement the required methods) that defines the game rules:\n",
        "# - get_initial_state(): returns the starting board state\n",
        "# - get_valid_moves(state): returns a boolean array of valid moves from the state\n",
        "# - get_next_state(state, action, player): returns the next state and player after an action\n",
        "# - get_game_ended(state, player): returns 1 for win, -1 for loss, 0 for draw, None if game continues\n",
        "# - get_canonical_form(state, player): returns state from the perspective of player 1\n",
        "# - get_symmetries(state, pi): returns a list of (state, policy) tuples representing board symmetries for data augmentation\n",
        "class TicTacToeGame:\n",
        "    \"\"\"Optimized Tic-Tac-Toe game implementation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.board_size = 3\n",
        "        self.action_size = 9  # 3x3 board\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros((3, 3), dtype=np.int8)\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        return (state == 0).flatten()\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "        next_state = state.copy()\n",
        "        row, col = action // 3, action % 3\n",
        "        next_state[row, col] = player\n",
        "        return next_state, -player  # Switch player\n",
        "\n",
        "    def get_game_ended(self, state, player):\n",
        "        # Check rows\n",
        "        for row in range(3):\n",
        "            if abs(sum(state[row])) == 3:\n",
        "                return sum(state[row]) / 3  # Returns 1 or -1\n",
        "\n",
        "        # Check columns\n",
        "        for col in range(3):\n",
        "            if abs(sum(state[:, col])) == 3:\n",
        "                return sum(state[:, col]) / 3\n",
        "\n",
        "        # Check diagonals\n",
        "        if abs(sum(state.diagonal())) == 3:\n",
        "            return sum(state.diagonal()) / 3\n",
        "        if abs(sum(np.fliplr(state).diagonal())) == 3:\n",
        "            return sum(np.fliplr(state).diagonal()) / 3\n",
        "\n",
        "        # Check for draw\n",
        "        if not (state == 0).any():\n",
        "            return 0\n",
        "\n",
        "        return None  # Game continues\n",
        "\n",
        "    def get_canonical_form(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_symmetries(self, state, pi):\n",
        "        \"\"\"Get all symmetries for data augmentation\"\"\"\n",
        "        symmetries = []\n",
        "\n",
        "        # Original\n",
        "        symmetries.append((state, pi))\n",
        "\n",
        "        # Rotations\n",
        "        for k in range(1, 4):\n",
        "            state_rot = np.rot90(state, k)\n",
        "            pi_rot = np.rot90(pi.reshape(3, 3), k).flatten()\n",
        "            symmetries.append((state_rot, pi_rot))\n",
        "\n",
        "        # Flips\n",
        "        state_flip = np.fliplr(state)\n",
        "        pi_flip = np.fliplr(pi.reshape(3, 3)).flatten()\n",
        "        symmetries.append((state_flip, pi_flip))\n",
        "\n",
        "        # Rotations of flipped\n",
        "        for k in range(1, 4):\n",
        "            state_rot = np.rot90(state_flip, k)\n",
        "            pi_rot = np.rot90(pi_flip.reshape(3, 3), k).flatten()\n",
        "            symmetries.append((state_rot, pi_rot))\n",
        "\n",
        "        return symmetries"
      ],
      "metadata": {
        "id": "ymCLwJHSdQek"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "npjJZOAAjpsx"
      },
      "outputs": [],
      "source": [
        "class ConnectFourGame:\n",
        "    \"\"\"Connect Four game implementation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.board_size = (self.rows, self.cols)\n",
        "        self.action_size = self.cols # Actions correspond to columns\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return np.zeros(self.board_size, dtype=np.int8)\n",
        "\n",
        "    def get_valid_moves(self, state):\n",
        "        # Valid moves are columns that are not full (top row is 0)\n",
        "        return state[0, :] == 0\n",
        "\n",
        "    def get_next_state(self, state, action, player):\n",
        "        next_state = state.copy()\n",
        "        # Find the lowest empty row in the chosen column\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            if next_state[r, action] == 0:\n",
        "                next_state[r, action] = player\n",
        "                break\n",
        "        return next_state, -player  # Switch player\n",
        "\n",
        "    def get_game_ended(self, state, player):\n",
        "        # Check for 4 in a row horizontally, vertically, and diagonally\n",
        "\n",
        "        # Check horizontal\n",
        "        for r in range(self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(state[r, c:c+4])) == 4:\n",
        "                    return sum(state[r, c:c+4]) / 4\n",
        "\n",
        "        # Check vertical\n",
        "        for c in range(self.cols):\n",
        "            for r in range(self.rows - 3):\n",
        "                if abs(sum(state[r:r+4, c])) == 4:\n",
        "                    return sum(state[r:r+4, c]) / 4\n",
        "\n",
        "        # Check diagonals (down-right)\n",
        "        for r in range(self.rows - 3):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(np.diagonal(state[r:r+4, c:c+4]))) == 4:\n",
        "                    return sum(np.diagonal(state[r:r+4, c:c+4])) / 4\n",
        "\n",
        "        # Check diagonals (up-right)\n",
        "        for r in range(3, self.rows):\n",
        "            for c in range(self.cols - 3):\n",
        "                if abs(sum(np.diagonal(np.fliplr(state[r-3:r+1, c:c+4])))) == 4:\n",
        "                    return sum(np.diagonal(np.fliplr(state[r-3:r+1, c:c+4]))) / 4\n",
        "\n",
        "        # Check for draw\n",
        "        if not (state[0, :] == 0).any(): # Check if the top row is full\n",
        "            return 0\n",
        "\n",
        "        return None  # Game continues\n",
        "\n",
        "    def get_canonical_form(self, state, player):\n",
        "        return state * player\n",
        "\n",
        "    def get_symmetries(self, state, pi):\n",
        "        \"\"\"Get symmetries for Connect Four (just horizontal flip)\"\"\"\n",
        "        # In Connect Four, only horizontal flipping is a relevant symmetry\n",
        "        symmetries = []\n",
        "\n",
        "        # Original\n",
        "        symmetries.append((state, pi))\n",
        "\n",
        "        # Horizontal flip\n",
        "        state_flip = np.fliplr(state)\n",
        "        pi_flip = np.fliplr(pi.reshape(1, self.cols)).flatten()\n",
        "        symmetries.append((state_flip, pi_flip))\n",
        "\n",
        "        return symmetries\n",
        "\n",
        "    def string_representation(self, state):\n",
        "        \"\"\"Returns a string representation of the board\"\"\"\n",
        "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
        "        board_str = \"\"\n",
        "        for r in range(self.rows):\n",
        "            board_str += \" \".join([symbols[state[r, c]] for c in range(self.cols)]) + \"\\n\"\n",
        "        board_str += \"--------------------\\n\"\n",
        "        board_str += \"0 1 2 3 4 5 6\"\n",
        "        return board_str\n",
        "\n",
        "# --- Neural Network ---\n",
        "# To extend to a new game, adjust the network architecture:\n",
        "# - Input shape: Match the new game's board representation.\n",
        "# - Output shapes: Match the new game's action space (policy head) and value output (value head).\n",
        "# - Consider different convolutional filter sizes or types if the board structure is very different.\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"Optimized neural network for AlphaZero\"\"\"\n",
        "\n",
        "    def __init__(self, game, num_channels=512):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.board_size = game.board_size\n",
        "        self.action_size = game.action_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        # Convolutional layers (adjust kernel size/padding if board size changes significantly)\n",
        "        self.conv1 = nn.Conv2d(1, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(num_channels, num_channels, 3, stride=1, padding=1)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn4 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "        # Policy head (adjust output size to game.action_size)\n",
        "        self.conv_policy = nn.Conv2d(num_channels, 32, 1)\n",
        "        self.bn_policy = nn.BatchNorm2d(32)\n",
        "        self.fc_policy = nn.Linear(32 * self.board_size[0] * self.board_size[1], self.action_size)\n",
        "\n",
        "        # Value head (adjust input size if board size changes)\n",
        "        self.conv_value = nn.Conv2d(num_channels, 3, 1)\n",
        "        self.bn_value = nn.BatchNorm2d(3)\n",
        "        self.fc_value1 = nn.Linear(3 * self.board_size[0] * self.board_size[1], 64)\n",
        "        self.fc_value2 = nn.Linear(64, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, s):\n",
        "        # Common layers\n",
        "        s = s.view(-1, 1, self.board_size[0], self.board_size[1])\n",
        "        s = F.relu(self.bn1(self.conv1(s)))\n",
        "        s = F.relu(self.bn2(self.conv2(s)))\n",
        "        s = F.relu(self.bn3(self.conv3(s)))\n",
        "        s = F.relu(self.bn4(self.conv4(s)))\n",
        "\n",
        "        # Policy head\n",
        "        pi = F.relu(self.bn_policy(self.conv_policy(s)))\n",
        "        pi = pi.view(pi.size(0), -1)\n",
        "        pi = self.dropout(pi)\n",
        "        pi = self.fc_policy(pi)\n",
        "        pi = F.log_softmax(pi, dim=1)\n",
        "\n",
        "        # Value head\n",
        "        v = F.relu(self.bn_value(self.conv_value(s)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = self.dropout(v)\n",
        "        v = F.relu(self.fc_value1(v))\n",
        "        v = self.fc_value2(v)\n",
        "        v = torch.tanh(v)\n",
        "\n",
        "        return pi, v\n",
        "\n",
        "# --- MCTS ---\n",
        "# The MCTS class is generally game-agnostic, relying on the Game and NeuralNetwork classes.\n",
        "# No major changes needed here for a new game unless the MCTS algorithm itself needs modification\n",
        "# (e.g., for games with very large branching factors).\n",
        "class MCTS:\n",
        "    \"\"\"Optimized Monte Carlo Tree Search\"\"\"\n",
        "\n",
        "    def __init__(self, game, nnet, args):\n",
        "        self.game = game\n",
        "        self.nnet = nnet\n",
        "        self.args = args\n",
        "        self.Qsa = {}  # Q values\n",
        "        self.Nsa = {}  # Visit counts for state-action\n",
        "        self.Ns = {}   # Visit counts for state\n",
        "        self.Ps = {}   # Policy\n",
        "        self.Es = {}   # Game ended\n",
        "        self.Vs = {}   # Valid moves\n",
        "\n",
        "    def get_action_prob(self, state, temp=1):\n",
        "        \"\"\"Get action probabilities and value estimate after MCTS simulations\"\"\"\n",
        "        for _ in range(self.args.num_mcts_sims):\n",
        "            self.search(state, 1)\n",
        "\n",
        "        s = self.game.get_canonical_form(state, 1)\n",
        "        # Use a more robust string representation for state keys\n",
        "        s_str = np.array2string(s, separator=',', max_line_width=np.inf)\n",
        "        counts = [self.Nsa.get((s_str, a), 0) for a in range(self.game.action_size)]\n",
        "\n",
        "        if temp == 0:\n",
        "            # Deterministic - choose best action\n",
        "            best_actions = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
        "            best_action = np.random.choice(best_actions)\n",
        "            probs = np.zeros(len(counts))\n",
        "            probs[best_action] = 1\n",
        "        else:\n",
        "            # Probabilistic based on temperature\n",
        "            counts = np.array(counts, dtype=np.float64)\n",
        "            counts = counts ** (1.0 / temp)\n",
        "            probs = counts / counts.sum()\n",
        "\n",
        "        # Calculate MCTS value estimate\n",
        "        # Weighted average of action values based on visit counts\n",
        "        total_visits = sum(counts)\n",
        "        if total_visits > 0:\n",
        "            mcts_value = 0\n",
        "            for a in range(self.game.action_size):\n",
        "                if (s_str, a) in self.Qsa and counts[a] > 0:\n",
        "                    mcts_value += (counts[a] / total_visits) * self.Qsa[(s_str, a)]\n",
        "        else:\n",
        "            mcts_value = 0\n",
        "\n",
        "        return probs, mcts_value\n",
        "\n",
        "    def search(self, state, player):\n",
        "        \"\"\"MCTS search with neural network guidance\"\"\"\n",
        "        s = self.game.get_canonical_form(state, player)\n",
        "        # Use a more robust string representation for state keys\n",
        "        s_str = np.array2string(s, separator=',', max_line_width=np.inf)\n",
        "\n",
        "\n",
        "        if s_str not in self.Es:\n",
        "            self.Es[s_str] = self.game.get_game_ended(s, 1) # Evaluate from player 1's perspective\n",
        "\n",
        "        if self.Es[s_str] is not None:\n",
        "            # Terminal node\n",
        "            return self.Es[s_str] # Return the game result directly\n",
        "\n",
        "        if s_str not in self.Ps:\n",
        "            # Leaf node - expand using neural network\n",
        "            with torch.no_grad():\n",
        "                # Ensure input tensor has the correct shape (batch, channels, height, width)\n",
        "                s_tensor = torch.FloatTensor(s).unsqueeze(0).unsqueeze(0).to(device)\n",
        "                log_pi, v = self.nnet(s_tensor)\n",
        "                pi = torch.exp(log_pi).cpu().numpy()[0]\n",
        "\n",
        "            valid_moves = self.game.get_valid_moves(s)\n",
        "            pi = pi * valid_moves  # Mask invalid moves\n",
        "            pi_sum = np.sum(pi)\n",
        "            if pi_sum > 0:\n",
        "                pi = pi / pi_sum  # Renormalize\n",
        "            else:\n",
        "                # If all valid moves have zero probability, distribute probability evenly\n",
        "                pi = valid_moves / np.sum(valid_moves)\n",
        "\n",
        "            self.Ps[s_str] = pi\n",
        "            self.Vs[s_str] = valid_moves\n",
        "            self.Ns[s_str] = 0\n",
        "            return v.item() # Return the value estimate\n",
        "\n",
        "        # Internal node - select action using UCB\n",
        "        valid_moves = self.Vs[s_str]\n",
        "        cur_best = -float('inf')\n",
        "        best_act = -1\n",
        "\n",
        "        # UCB formula\n",
        "        for a in range(self.game.action_size):\n",
        "            if valid_moves[a]:\n",
        "                if (s_str, a) in self.Qsa:\n",
        "                    u = (self.Qsa[(s_str, a)] +\n",
        "                         self.args.cpuct * self.Ps[s_str][a] *\n",
        "                         math.sqrt(self.Ns[s_str]) / (1 + self.Nsa[(s_str, a)]))\n",
        "                else:\n",
        "                    u = (self.args.cpuct * self.Ps[s_str][a] *\n",
        "                         math.sqrt(self.Ns[s_str] + 1e-8)) # Add epsilon to prevent division by zero\n",
        "\n",
        "                if u > cur_best:\n",
        "                    cur_best = u\n",
        "                    best_act = a\n",
        "\n",
        "        # Make move and recurse\n",
        "        next_s, next_player = self.game.get_next_state(state, best_act, player) # Use original state here\n",
        "        v = self.search(next_s, next_player)\n",
        "\n",
        "        # Backup\n",
        "        if (s_str, best_act) in self.Qsa:\n",
        "            self.Qsa[(s_str, best_act)] = ((self.Nsa[(s_str, best_act)] *\n",
        "                                          self.Qsa[(s_str, best_act)] + v) /\n",
        "                                         (self.Nsa[(s_str, best_act)] + 1))\n",
        "            self.Nsa[(s_str, best_act)] += 1\n",
        "        else:\n",
        "            self.Qsa[(s_str, best_act)] = v\n",
        "            self.Nsa[(s_str, best_act)] = 1\n",
        "\n",
        "        self.Ns[s_str] += 1\n",
        "        return v\n",
        "\n",
        "# --- AlphaZero Trainer ---\n",
        "# The trainer orchestrates self-play, training, and evaluation.\n",
        "# Mostly game-agnostic, but requires the Game, NeuralNetwork, MCTS, and Args classes.\n",
        "class AlphaZeroTrainer:\n",
        "    \"\"\"Main AlphaZero trainer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.game = ConnectFourGame()\n",
        "        self.args = Args()\n",
        "        self.nnet = NeuralNetwork(self.game).to(device)\n",
        "        self.optimizer = optim.Adam(self.nnet.parameters(), lr=self.args.lr,\n",
        "                                   weight_decay=self.args.weight_decay)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
        "        self.scaler = GradScaler('cuda')\n",
        "\n",
        "        self.train_examples_history = deque([], maxlen=self.args.max_memory_size // self.args.num_eps) # Store examples from recent episodes\n",
        "\n",
        "        # Heap: (-elo, iteration, model_copy)\n",
        "        self.model_heap = []\n",
        "        heapq.heappush(self.model_heap, (-1000, 0, copy.deepcopy(self.nnet)))  # Iter 0, base net\n",
        "\n",
        "\n",
        "    def execute_episode(self):\n",
        "        \"\"\"Execute one episode of self-play\"\"\"\n",
        "        train_examples = []\n",
        "        state = self.game.get_initial_state()\n",
        "        current_player = 1\n",
        "        episode_step = 0\n",
        "\n",
        "        while True:\n",
        "            episode_step += 1\n",
        "            canonical_state = self.game.get_canonical_form(state, current_player)\n",
        "            temp = int(episode_step < self.args.temp_threshold)\n",
        "\n",
        "            # Get action probabilities from MCTS\n",
        "            mcts = MCTS(self.game, self.nnet, self.args)\n",
        "            pi, _ = mcts.get_action_prob(canonical_state, temp=temp)\n",
        "\n",
        "            # Store training example (value target will be filled in later)\n",
        "            sym = self.game.get_symmetries(canonical_state, pi)\n",
        "            for s, p in sym:\n",
        "                train_examples.append([s, current_player, p])  # no value yet\n",
        "\n",
        "            # Sample action from probabilities\n",
        "            action = np.random.choice(len(pi), p=pi)\n",
        "\n",
        "            # Make move\n",
        "            state, current_player = self.game.get_next_state(state, action, current_player)\n",
        "\n",
        "            # Check if game ended\n",
        "            result = self.game.get_game_ended(state, current_player)\n",
        "            if result is not None:\n",
        "                # Assign final outcome z to all training examples\n",
        "                final_examples = []\n",
        "                for s, player, p in train_examples:\n",
        "                    z = result * player  # flip perspective depending on who played\n",
        "                    final_examples.append([s, player, p, z])\n",
        "                return final_examples\n",
        "\n",
        "    def train(self, examples):\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        random.shuffle(examples)\n",
        "\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        self.nnet.train() # Set model to training mode\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            batch_idx = 0\n",
        "\n",
        "            while batch_idx < len(examples):\n",
        "                batch_examples = examples[batch_idx:batch_idx + self.args.batch_size]\n",
        "                batch_idx += self.args.batch_size\n",
        "\n",
        "                # Ensure input tensor has the correct shape (batch, channels, height, width)\n",
        "                boards = torch.FloatTensor(np.array([ex[0] for ex in batch_examples])).unsqueeze(1).to(device)\n",
        "                target_pis = torch.FloatTensor(np.array([ex[2] for ex in batch_examples])).to(device)\n",
        "                target_vs = torch.FloatTensor(np.array([ex[3] for ex in batch_examples])).to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                with autocast('cuda'):\n",
        "                    out_pi, out_v = self.nnet(boards)\n",
        "                    l_pi = self.loss_pi(target_pis, out_pi)\n",
        "                    l_v = self.loss_v(target_vs, out_v)\n",
        "                    total_loss_batch = l_pi + l_v\n",
        "\n",
        "                self.scaler.scale(total_loss_batch).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                total_loss += total_loss_batch.item()\n",
        "                num_batches += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "        avg_loss = total_loss / num_batches\n",
        "        return avg_loss\n",
        "\n",
        "    def loss_pi(self, targets, outputs):\n",
        "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
        "\n",
        "    def loss_v(self, targets, outputs):\n",
        "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
        "\n",
        "    def play_game(self, nnet1, nnet2):\n",
        "        \"\"\"Play a single game between two nets. Returns 1 if nnet1 wins, 0 if nnet2 wins, 0.5 if draw.\"\"\"\n",
        "        state = self.game.get_initial_state()\n",
        "        player = 1\n",
        "        while True:\n",
        "            nnet = nnet1 if player == 1 else nnet2\n",
        "            mcts = MCTS(self.game, nnet, self.args)\n",
        "            pi, _ = mcts.get_action_prob(self.game.get_canonical_form(state, player), temp=0)\n",
        "            action = np.argmax(pi)\n",
        "            state, player = self.game.get_next_state(state, action, player)\n",
        "\n",
        "            result = self.game.get_game_ended(state, player)\n",
        "            if result is not None:\n",
        "                if result == 0:\n",
        "                    return 0.5\n",
        "                return 1 if result == 1 else 0  # 1 means player 1 (nnet1) wins, -1 means nnet2 wins\n",
        "\n",
        "    def update_elo(self, elo_a, elo_b, score_a, k=32):\n",
        "        \"\"\"Update Elo ratings after a game. score_a is 1 if A wins, 0.5 for draw, 0 if A loses.\"\"\"\n",
        "        expected_a = 1 / (1 + 10 ** ((elo_b - elo_a) / 400))\n",
        "        expected_b = 1 - expected_a\n",
        "        new_elo_a = elo_a + k * (score_a - expected_a)\n",
        "        new_elo_b = elo_b + k * ((1 - score_a) - expected_b)\n",
        "        return new_elo_a, new_elo_b\n",
        "\n",
        "    def evaluate_models(self, new_model, iteration):\n",
        "        # Pop all previous models from the heap\n",
        "        previous_models = []\n",
        "        while self.model_heap:\n",
        "            previous_models.append(heapq.heappop(self.model_heap))\n",
        "\n",
        "        new_elo = 1000  # starting Elo for new model\n",
        "\n",
        "        # Play two games against each previous model: once as first, once as second\n",
        "        for i, (neg_elo, iter_idx, old_model) in enumerate(previous_models):\n",
        "            # Game 1: new_model goes first\n",
        "            score1 = self.play_game(new_model, old_model)\n",
        "            new_elo, old_elo = self.update_elo(new_elo, -neg_elo, score1)\n",
        "\n",
        "            # Game 2: new_model goes second\n",
        "            score2 = self.play_game(old_model, new_model)\n",
        "            # Swap score for perspective of new_model\n",
        "            new_elo, old_elo = self.update_elo(new_elo, old_elo, 1 - score2)\n",
        "\n",
        "            # Update the heap tuple with the new opponent Elo\n",
        "            previous_models[i] = (-old_elo, iter_idx, old_model)\n",
        "\n",
        "        # Push all previous models back into the heap\n",
        "        for model in previous_models:\n",
        "            heapq.heappush(self.model_heap, model)\n",
        "\n",
        "        # Push new model into heap\n",
        "        heapq.heappush(self.model_heap, (-new_elo, iteration, new_model))\n",
        "\n",
        "        # Optionally, set self.nnet to the model with highest Elo\n",
        "        top_neg_elo, _, top_model = max(self.model_heap)\n",
        "        self.nnet = top_model\n",
        "\n",
        "        return new_elo\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(\"Starting AlphaZero training...\")\n",
        "\n",
        "        for i in range(1, self.args.num_iters + 1):\n",
        "            print(f\"\\n=== Iteration {i}/{self.args.num_iters} ===\")\n",
        "\n",
        "            # Self-play\n",
        "            iteration_train_examples = deque([], maxlen=self.args.max_memory_size)\n",
        "\n",
        "            print(\"Collecting self-play games...\")\n",
        "            for eps in range(self.args.num_eps):\n",
        "                iteration_train_examples.extend(self.execute_episode()) # Use extend\n",
        "                if (eps + 1) % 10 == 0: # Print progress less frequently\n",
        "                    print(f\"Episode {eps + 1}/{self.args.num_eps}\")\n",
        "\n",
        "            # Add to training history\n",
        "            self.train_examples_history.append(iteration_train_examples)\n",
        "\n",
        "            # Prepare training data from history\n",
        "            train_examples = []\n",
        "            for e in self.train_examples_history:\n",
        "                train_examples.extend(e)\n",
        "\n",
        "            # Train\n",
        "            print(f\"Training on {len(train_examples)} examples...\")\n",
        "            avg_loss = self.train(train_examples)\n",
        "\n",
        "            # Save and evaluate\n",
        "            new_model = copy.deepcopy(self.nnet)\n",
        "            elo = self.evaluate_models(new_model, i)\n",
        "\n",
        "            print(\"\\n--- Elo Leaderboard ---\")\n",
        "            # Convert heap into a sorted list by Elo (highest first)\n",
        "            sorted_models = sorted(self.model_heap, key=lambda x: x[0])  # x[0] is -elo\n",
        "            for neg_elo, iteration, _ in sorted_models:\n",
        "                print(f\"Iteration {iteration}: {-neg_elo:.2f}\")\n",
        "            print(\"-----------------------\\n\")\n",
        "\n",
        "            # Pick the best model (highest Elo = smallest neg_elo in heap)\n",
        "            neg_elo, best_iter, best_model = self.model_heap[0]\n",
        "\n",
        "            # Update current nnet to the best model\n",
        "            self.nnet = best_model\n",
        "\n",
        "            # Reinitialize optimizer and scaler for the new model\n",
        "            self.optimizer = torch.optim.Adam(self.nnet.parameters(), lr=self.args.lr)\n",
        "            self.scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "        return self.model_heap\n",
        "\n",
        "def demo_game():\n",
        "    \"\"\"Demo game between trained AlphaZero and Optimal AI and evaluate specific state\"\"\"\n",
        "    print(\"\\n=== Demo Game and Evaluation ===\")\n",
        "    trainer = AlphaZeroTrainer()\n",
        "\n",
        "    # Quick training for demo (reduce iterations for faster demo)\n",
        "    trainer.args.num_iters = 100 # Reduced iterations\n",
        "    trainer.args.num_eps = 20 # Reduced episodes\n",
        "    trainer.args.num_mcts_sims = 250 # Reduced simulations per move\n",
        "\n",
        "    print(\"Training AlphaZero (reduced for demo)...\")\n",
        "    model_heap = trainer.learn()\n",
        "\n",
        "    return model_heap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Arguments ---\n",
        "# Adjust these parameters based on the complexity of the new game.\n",
        "# More complex games may require more simulations, episodes, iterations, and a larger batch size.\n",
        "class Args:\n",
        "    \"\"\"Training arguments\"\"\"\n",
        "    def __init__(self):\n",
        "        # Self-play\n",
        "        self.num_iters = 100\n",
        "        self.num_eps = 50  # Episodes per iteration\n",
        "        self.temp_threshold = 15 # Number of moves to use temperature sampling in self-play\n",
        "\n",
        "        # MCTS\n",
        "        self.num_mcts_sims = 250 # Number of MCTS simulations per move\n",
        "        self.cpuct = 1.0 # Exploration constant\n",
        "\n",
        "        # Training\n",
        "        self.epochs = 50\n",
        "        self.batch_size = 512\n",
        "        self.lr = 0.001\n",
        "        self.dropout = 0.3\n",
        "        self.weight_decay = 1e-4\n",
        "\n",
        "        # Memory\n",
        "        self.max_memory_size = 100000 # Maximum number of training examples\n",
        "\n",
        "        # Evaluation\n",
        "        self.arena_compare = 40 # Number of games to compare new vs old network\n",
        "        self.update_threshold = 0.6 # Minimum win rate against old network to update"
      ],
      "metadata": {
        "id": "14prAVYId2yJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the demo\n",
        "    demo_game()"
      ],
      "metadata": {
        "id": "pXKHqICBwMw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b002ba9-39eb-4888-edd0-0c958ccf16cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Demo Game and Evaluation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training AlphaZero (reduced for demo)...\n",
            "Starting AlphaZero training...\n",
            "\n",
            "=== Iteration 1/100 ===\n",
            "Collecting self-play games...\n",
            "Episode 10/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYtziK4ovQmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
